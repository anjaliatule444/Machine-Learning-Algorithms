{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "X,Y=load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients= [  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
      "Intercept= 151.88331005254167\n",
      "Mean Absolute Error= 45.21303419046903\n",
      "Mean Squared Error= 3094.4566715660626\n",
      "Root Mean Squared Error= 55.627840795469155\n",
      "R2 Score= 0.4399338661568968\n"
     ]
    }
   ],
   "source": [
    "## Apply Simple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "LR=LinearRegression()\n",
    "\n",
    "## Fit\n",
    "LR.fit(X_train,Y_train)\n",
    "\n",
    "## Print Coefficients and Intercepts\n",
    "print(\"Coefficients=\",LR.coef_)\n",
    "print(\"Intercept=\",LR.intercept_)\n",
    "\n",
    "#Prediction\n",
    "Y_pred=LR.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "mae=mean_absolute_error(Y_test,Y_pred)\n",
    "mse=mean_squared_error(Y_test,Y_pred)\n",
    "rmse=np.sqrt(mse)\n",
    "print(\"Mean Absolute Error=\",mae)\n",
    "print(\"Mean Squared Error=\",mse)\n",
    "print(\"Root Mean Squared Error=\",rmse)\n",
    "\n",
    "score=r2_score(Y_test,Y_pred)\n",
    "print(\"R2 Score=\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stochastic Gradient Descent Function\n",
    "\n",
    "class SGDRegressor:\n",
    "\n",
    "    ## Constructor\n",
    "    def __init__(self,learning_rate=0.01,epochs=10):\n",
    "        self.coefficient_=None\n",
    "        self.intercept_=None\n",
    "        self.lr=learning_rate\n",
    "        self.epochs=epochs\n",
    "\n",
    "    ## Fit method\n",
    "    def fit(self,X_train,Y_train):\n",
    "        ## Initialize coefficients i.e. Intercept to 0 and coeffients will be 10 for 10 columns initialized to 1\n",
    "        self.intercept_=0\n",
    "        self.coefficient_=np.ones(X_train.shape[1])\n",
    "\n",
    "        ## In each epoch for each rows coefficients will get updated\n",
    "        for i in range(self.epochs): \n",
    "            for j in range(X_train.shape[0]): ## X_train.shape[0]-->353 rows\n",
    "                idx=np.random.randint(0,X_train.shape[0]) ## In SGD rows are picked up randomly so generating a random row number each time\n",
    "                \n",
    "                y_hat=np.dot(X_train[idx],self.coefficient_)+self.intercept_ ## Same as y_hat=mx+c for each row and here y_hat is a single number\n",
    "                \n",
    "                ## Calculation of intercept derivative for updating intercept\n",
    "                intercept_der=-2 * (Y_train[idx]-y_hat)\n",
    "\n",
    "                ## Update intercept based on covergence algorithm\n",
    "                self.intercept_=self.intercept_-(self.lr * intercept_der)\n",
    "\n",
    "                ## Calculation of coeffient derivative\n",
    "                coef_der = -2 * np.dot((Y_train[idx] - y_hat),X_train[idx])\n",
    "\n",
    "                ## Update intercept based on covergence algorithm\n",
    "                self.coefficient_=self.coefficient_ - (self.lr * coef_der)\n",
    "\n",
    "        print(\"Coefficients=\",self.coefficient_)\n",
    "        print(\"Intercept=\",self.intercept_)\n",
    "\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coefficient_) + self.intercept_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(learning_rate=0.01,epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape ## (353,)\n",
    "#Y_train.reshape(X_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients= [  49.82657493  -94.10194419  379.20139126  275.58070889   14.37527351\n",
      "  -42.58163346 -181.07957223  136.75111061  355.68256538  120.56102506]\n",
      "Intercept= 145.71490133496943\n",
      "Time Taken= 0.9095494747161865\n"
     ]
    }
   ],
   "source": [
    "## We will fit the data and also will understand the time taken\n",
    "import time as t\n",
    "\n",
    "start=t.time()\n",
    "sgd.fit(X_train,Y_train)\n",
    "print(\"Time Taken=\",t.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed above time taken is very less for stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "Y_pred=sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score= 0.44025953492688696\n"
     ]
    }
   ],
   "source": [
    "## Score\n",
    "score=r2_score(Y_test,Y_pred)\n",
    "print(\"Score=\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everytime we run the algorithm we get little varied result of coefficient and learning rate as the rows are picked randomely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below helpers Used for writing SGDRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train.shape ## (353,10)\n",
    "#X_train.shape[1] ## 10\n",
    "np.ones(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This will generate random number between 0 and 353 each time\n",
    "np.random.randint(0,X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    0.056239\n",
       "sex   -0.044642\n",
       "bmi   -0.057941\n",
       "bp    -0.007977\n",
       "s1     0.052093\n",
       "s2     0.049103\n",
       "s3     0.056003\n",
       "s4    -0.021412\n",
       "s5    -0.028323\n",
       "s6     0.044485\n",
       "Name: 200, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09762897536168796"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X_train.iloc[42],np.ones(X_train.shape[1]))+0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import SGD Regressor\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max_iter=epochs\n",
    "## eta0= initial value of learning rate\n",
    "## Learning rate is constant i.e. eta0\n",
    "reg=SGDRegressor(max_iter=100,learning_rate='constant',eta0=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1527: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(learning_rate=&#x27;constant&#x27;, max_iter=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(learning_rate=&#x27;constant&#x27;, max_iter=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(learning_rate='constant', max_iter=100)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score= 0.432942487026887\n"
     ]
    }
   ],
   "source": [
    "print(\"Score=\",r2_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
